# Session Log — 2026-02-17

## Context
- President's Day (Feb 16) — market closed, no new data for that day
- Trader v29 running with Feb 16 watchlist (7 symbols: FKU, FLQS, GLTO, CNVS, VTAK, GP, FCA)
- Previous session (Feb 16) completed: Neo4j password fix, FastAPI systemd service, PalantirGatekeeper wired into v29, macro context cron job

## Tasks Completed

### 1. World Knowledge Base — Project Exploration
- Extracted `global_financial_kb (1).tar.gz` from `/media/peter/fast-storage/projects/world_knowledge_base/`
- Confirmed codebase: 7,324 lines, 16 files — fully built in a prior session with Gemini/Claude
- Fixed `DEV_DATA_ROOT` in `config/settings.py` (was `/home/claude/...`, now uses project-relative `data/`)
- Installed missing dependency: `hmmlearn`
- Confirmed all modes work: `--mode hmm-test`, `--mode graph-only`, `--mode full`
- Discovered existing `financial_kg.json` (436KB) with 2,220 nodes and 2,398 edges

### 2. Phase 1 — Parallel Async Ingestion (32× speedup)
**Problem:** Original ingestion was serial — `for stat: for country: await fetch()` — estimated 8+ hours for 86 stats × 20 countries. Confirmed by running old version (55 min, only finished 2/86 stats before kill).

**Fix:** Rewrote `ingestion/pipeline.py`:
- `asyncio.gather(*all_tasks)` — all fetches run concurrently
- Per-host semaphores: World Bank=3, FRED=8 concurrent connections
- Timeout: 30s → 15s (fail fast)
- `asyncio.Lock()` for thread-safe accumulation
- Staleness cache: skips re-fetching data < N days old (WB=30 days, FRED=1 day)
- Progress logging every 25 tasks

**Result:** 15,105 observations collected in **14 minutes 57 seconds** (was 8+ hours projected)

### 3. Phase 2 — Lambda Build Script
Created `build_initial_kb.py` — self-contained 6-phase build script for Lambda Labs A6000/A100:
- Phase 1: Full parallel ingestion (`skip_cache=True`)
- Phase 2: LLM stat inference — 17 LLM-required stats × 20 countries, batched (5 stats/call → ~80 API calls vs 340)
- Phase 3: Edge weight inference — 31 trade corridors × 6 stat pairs (~31 calls vs 7,000)
- Phase 4: Full graph construction with LLM-blended contagion weights
- Phase 5: Baum-Welch HMM training per country (all 20 converged)
- Phase 6: Pack `kb_snapshot_YYYYMMDD.tar.gz` for rsync download

Features:
- `--resume` flag: checkpoint saves after each country, survives Lambda preemption
- `--skip-llm`: API-data-only build for testing
- `--skip-ingestion --resume`: use existing parquet, jump to graph phases
- `LLM_MODE=local|claude`: switch between vLLM (Lambda) and Claude API (dev)

Created `setup_lambda.sh` — one-shot Lambda environment setup:
- Installs Python deps + vLLM
- Auto-detects GPU VRAM: A6000/A100 → Llama-3.1-70B-AWQ-INT4, smaller → Mistral-7B
- Starts vLLM server in background, waits for ready (up to 15 min for model download)
- Prints rsync upload/download commands

**Tested:** Build runs end-to-end in 6 seconds locally (--skip-llm, cached data). All 20 countries HMM-trained, snapshot exported.

### 4. Git & GitHub
- Initialized new git repo in `global_financial_kb/`
- Created GitHub repo: **https://github.com/kayakpeter/world_knowledge_base**
- Two commits pushed:
  - `75b1255` — initial commit (Phase 1)
  - `49999dc` — Phase 2 Lambda build

### 5. Architecture Discussion
Had extended design session on the World KB roadmap. Key decisions:
- **Option D** — all three output types:
  - A: `scanner_signals.json` → Greenshoe scanner reads at market open
  - B: `daily_risk_report.md` → human review (email/WhatsApp)
  - C: threshold alerts → push when scenario Δ > 5pp or HMM state changes
- **Two-machine architecture:**
  - Lambda (one-time/weekly): full ingestion + LLM edge weights + Baum-Welch
  - Home system (daily): incremental update, Claude API for small LLM calls, signal generation
- **rsync** for data transfer, Lambda persistent storage for parquet cache
- **Revenue Canada case**: Lambda struggles = documented proof for hardware upgrade (A6000 × 3-4 or A100)
- Future phases discussed: sub-national breakdown, dynamic scenario management, stock exchange integration (North American actionability filter), data source intelligence agent

## Key Files Created/Modified
| File | Action | Notes |
|---|---|---|
| `global_financial_kb/ingestion/pipeline.py` | Rewritten | Serial → parallel async, 32× faster |
| `global_financial_kb/build_initial_kb.py` | Created | Lambda 6-phase build script |
| `global_financial_kb/setup_lambda.sh` | Created | One-shot Lambda environment setup |
| `global_financial_kb/config/settings.py` | Fixed | DEV_DATA_ROOT path |
| `global_financial_kb/.gitignore` | Created | Excludes parquet data, pyc, tar.gz |

## Data Locations
- **Raw observations:** `global_financial_kb/data/raw/observations_20260217_155028.parquet` (76KB, 15,105 rows)
- **Staleness cache:** `global_financial_kb/data/raw/staleness_cache.parquet`
- **Test snapshot:** `global_financial_kb/data/build/snapshots/kb_snapshot_20260217.tar.gz`
- **Episode 001 script:** `global_financial_kb/data/briefings/episode_001/episode_001_script.md`

## HMM State Estimates (Feb 17, sparse data)
Notable even with limited observations:
- Japan → **Crisis** (BoJ policy rate trajectory)
- Canada → **Turbulent** (USMCA/tariff exposure)
- Russia → **Turbulent**
- India → **Turbulent**
- US → Tranquil (but US has 2,245 obs — most reliable estimate)

## Session 2 — Lambda Build Run (same day, continuation)

### 6. Lambda Instance Setup
- Spun up `gpu_1x_a100_sxm4` in us-west-2 (us-east-1 had insufficient capacity)
- Instance ID: `0843f1fa0e3549ba9c4f43c1f875b767` | IP: `129.146.71.49` | Cost: $1.29/hr
- Added `129.146.71.49` to `~/.ssh/known_hosts`
- Uploaded project code via rsync (228 files, excludes parquet/tar.gz/pyc)

### 7. vLLM Environment Fix
**Problem:** `setup_lambda.sh` installed vLLM via `pip install --user`, but system TensorFlow (compiled for numpy 1.x) conflicts with vLLM's numpy 2.x. `transformers/image_transforms.py` unconditionally imports TF → crash on startup.

**Fix:** Created isolated virtualenv (`~/venv`), copied user site-packages into it, ran `pip install vllm ...` inside venv to resolve all deps cleanly. Isolated from system TF.

**70B model OOM:** `hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4` weights consume ~37-38 GB on A100 40GB, leaving no room for vLLM KV cache blocks. Tried both `--gpu-memory-utilization 0.90` (OOM during torch.inductor autotuning compilation) and `0.85 + --enforce-eager` (KV cache check fails — 0 blocks available). **A100 40GB cannot run 70B AWQ + KV cache.**

**Workaround:** Switched to `hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4`:
- ~5 GB weights, 34 GB free for KV cache
- Fits easily, CUDA graphs compiled in 2.5s
- Server healthy at `http://localhost:8000/health`

**Also fixed:** Updated `processing/llm_interface.py` default model from `meta-llama/Llama-3.3-70B-Instruct` → `hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4` on Lambda.

### 8. Full KB Build — Lambda Run
Launched `build_initial_kb.py` with `LLM_MODE=local`, `FRED_API_KEY` set.

| Phase | Result | Time |
|---|---|---|
| Phase 1: Parallel ingestion | 18,727 observations (1,720 tasks) | 44.5 min (WB throttling slowed tail) |
| Phase 2: LLM stat inference | 2,958 LLM-inferred data points | ~2.5 min |
| Phase 3: Edge weight inference | 186 cross-border edges | ~2.5 min |
| Phase 4: Graph construction | 2,220 nodes / 2,552 edges | <1 min |
| Phase 5: HMM Baum-Welch | All 20 countries trained | ~14 sec |
| Phase 6: Snapshot export | `kb_snapshot_20260217.tar.gz` | <1 sec |
| **Total** | | **49.4 minutes / ~$1.06** |

Snapshot downloaded to local machine. Lambda instance terminated.

## Data Locations (Updated)
- **Lambda snapshot (production build):** `/media/peter/fast-storage/projects/world_knowledge_base/snapshots/kb_20260217/`
  - `financial_kg.json` — 980 KB, 2,220 nodes, 2,552 edges
  - `hmm_params.json` — 20 countries trained
  - `observations.parquet` — 18,727 rows
  - `llm_stats.parquet` — 2,958 rows
  - `edge_weights.parquet` — 186 cross-border edges
  - `manifest.json` — build metadata
- **Tar.gz:** `/media/peter/fast-storage/projects/world_knowledge_base/snapshots/kb_snapshot_20260217.tar.gz`
- **Local dev snapshot:** `global_financial_kb/data/build/snapshots/kb_snapshot_20260217.tar.gz`
- **Episode 001 script:** `global_financial_kb/data/briefings/episode_001/episode_001_script.md`

## Key Learnings for Future Lambda Runs
- **70B model requires A6000 48GB or A100 80GB** — A100 40GB cannot hold model + KV cache
- **vLLM must run in a venv** — system TF (numpy 1.x compiled) conflicts with vLLM's numpy 2.x
- **`setup_lambda.sh` needs updates** before next run:
  - Create venv instead of `pip install --user`
  - Use 8B model by default for 40GB instances; 70B for ≥48GB
  - `meta-llama/` models are gated — use `hugging-quants/` mirrors
- **World Bank rate-limits Lambda IPs more aggressively** than home IP — last ~14% of ingestion takes ~30 min instead of ~5 min
- **Lambda API (Cloudflare-protected) blocks programmatic termination from home IP** — use web dashboard

## Session 3 — Missing Fetchers Build (same day, continuation)

### 9. IMF, OECD, BIS, EIA Fetchers
**Context:** KB scope assessment revealed ~60% of designed data was missing — all 4 non-WB/FRED providers were stub `return []` in `_dispatch_fetch`. Built all four fetchers plus wired into pipeline. Commit: `71eda8b`.

**Files created:**
| File | Provider | Status |
|---|---|---|
| `ingestion/fetchers/imf.py` | IMF DataMapper API | **Live — 3 stats working** |
| `ingestion/fetchers/oecd.py` | OECD SDMX-JSON | **Graceful empty — API migration** |
| `ingestion/fetchers/bis.py` | BIS Statistics API | **Graceful empty — server bug** |
| `ingestion/fetchers/eia.py` | EIA International | **Future-ready — no stats registered yet** |

**IMF fetcher (imf.py):**
- Endpoint: `https://www.imf.org/external/datamapper/api/v1/{indicator}`
- No auth; one API call returns all 20 countries at once — per-indicator in-process cache avoids N×20 calls
- Uses ISO-3 codes (confirmed — NOT the numeric IMF codes stored in COUNTRY_CODES["imf"])
- **3 stats validated live:**
  - `debt_to_gdp` → `GGXWDG_NGDP` ✓
  - `primary_deficit` → `GGXONLB_G01_GDP_PT` ✓ (old code `GGXONLB_NGDP` doesn't exist on DataMapper)
  - `output_gap` → `GGXCNL_NGDP` ✓ (repurposed as overall fiscal balance proxy)
- `public_investment_ratio` moved to **World Bank `NE.GDI.TOTL.ZS`** — IMF DataMapper investment stats are Africa-only

**OECD fetcher (oecd.py):**
- OECD migrated from `stats.oecd.org` → `sdmx.oecd.org` in 2024-2025
- Old API returns 301 redirect that **drops the dimension key** → fetcher gets root dataset (all countries/years) → 200 but no country-filtered data parseable
- New endpoint returns 422 for all dimension key formats tested
- Fetcher tries new endpoint first, then old, skips 301 redirects gracefully
- `tax_to_gdp` moved to **World Bank `GC.TAX.TOTL.GD.ZS`** — OECD API broken
- `entitlement_spend` remains OECD (`api_provider="oecd"`) — returns [] until API stabilises

**BIS fetcher (bis.py):**
- Correct dataset: `WS_CBPOL` (not `WS_CBPOL_D` as originally assumed)
- BIS data endpoint returns 500 "Could not find structure type with class 'WS_CBPOL'" despite dataset appearing in `/structure/dataflow` — server-side bug
- Health check uses `/structure/dataflow` instead (returns 200)
- Added graceful 500 handling — returns [] without error logging spam

**Policy rate expansion:**
- `policy_rate.fred_series` expanded from 4 → 17 countries:
  - ECB zone: DEU, FRA, ITA, ESP, NLD → `ECBMLFR` (shared ECB main rate)
  - OECD series: AUS, KOR, MEX, TUR, POL, CHN, IND, BRA → `IRSTCB01{ISO2}M156N`
  - Missing 3 (IDN, RUS, SAU): BIS fallback wired, returns [] until BIS API fixed

**Pipeline updates (`ingestion/pipeline.py`):**
- 6 new semaphores: IMF=5, OECD=3, BIS=5, EIA=5
- `_dispatch_fetch` routes all 4 new providers
- FRED stats with `bis_indicator` fall back to BIS when country not in `fred_series`
- Staleness days: IMF=30, OECD=30, BIS=1, EIA=30

**Settings updates (`config/settings.py`):**
- `StatDefinition` gained 3 new optional fields: `oecd_indicator`, `bis_indicator`, `eia_indicator`
- Provider breakdown after changes: WB=35, FRED=47, IMF=3, OECD=1

## Key Files Created/Modified
| File | Action | Notes |
|---|---|---|
| `global_financial_kb/ingestion/pipeline.py` | Rewritten | Serial → parallel async, 32× faster |
| `global_financial_kb/build_initial_kb.py` | Created | Lambda 6-phase build script |
| `global_financial_kb/setup_lambda.sh` | Created | One-shot Lambda environment setup |
| `global_financial_kb/config/settings.py` | Fixed + Updated | DEV_DATA_ROOT path; new stat fields; corrected IMF codes; expanded policy_rate |
| `global_financial_kb/.gitignore` | Created | Excludes parquet data, pyc, tar.gz |
| `global_financial_kb/ingestion/fetchers/imf.py` | Created | IMF DataMapper fetcher (live) |
| `global_financial_kb/ingestion/fetchers/oecd.py` | Created | OECD SDMX fetcher (graceful empty) |
| `global_financial_kb/ingestion/fetchers/bis.py` | Created | BIS fetcher (graceful empty) |
| `global_financial_kb/ingestion/fetchers/eia.py` | Created | EIA fetcher (future-ready) |

## Session 4 — 70B Lambda Build on H100 (Feb 17–18, continuation)

### 10. Pre-flight Fixes
Three files fixed before Lambda run:

**`build_initial_kb.py`** — `--skip-ingestion` bug fixed:
- Was: `if args.skip_ingestion and args.resume:` — required `--resume` to also be passed
- Now: `if args.skip_ingestion:` — works standalone; errors with clear message if no parquet found

**`setup_lambda.sh`** — complete rewrite:
- VRAM threshold raised from `≥40000` → `≥48000` MiB (A100 40GB cannot hold 70B AWQ + KV cache)
- Step 3: creates `~/venv` virtualenv to isolate from system TF/numpy conflict
- All pip installs run inside venv
- Models switched to ungated `hugging-quants/` mirrors
- vLLM launched via `~/venv/bin/python`
- Wait timeout extended to 1500s (25 min for model download)
- Progress logging: prints last vllm_server.log line every 30s during wait

**`processing/llm_interface.py`** — default local model:
- Was: `"meta-llama/Llama-3.3-70B-Instruct"` (gated, requires HF login)
- Now: `"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"` (ungated mirror)

### 11. Local Ingestion (Home Machine)
Ran `python build_initial_kb.py --skip-llm` on home machine to pre-fetch all API data:
- **36,869 observations** in **75.7 seconds** (vs 44.5 min on Lambda's throttled IP)
- Output: `data/raw/observations_20260217_203523.parquet`
- 17 LLM-inferred stats deferred to Lambda

### 12. H100 Instance — Broken Instance (192.222.55.238)
First H100 instance (`gpu_1x_h100_sxm5`, 80GB SXM5) had persistent CUDA error 802 (`cudaErrorSystemNotReady`):
- `nvidia-smi` worked (NVML path), but `cuInit()` returned 802 — driver-level failure
- NVIDIA Fabric Manager couldn't start (`NV_WARN_NOTHING_TO_DO` — no NVSwitch present)
- UVM module reload caused `cuInit` to hang indefinitely
- Persisted across clean reboot — Lambda-side hypervisor/VFIO passthrough misconfiguration
- **Resolution:** Terminated instance, launched fresh one

### 13. H100 Instance — Working Instance (209.20.158.148)
Second instance: `gpu_1x_h100_pcie` (H100 PCIe 80GB)
- CUDA smoke test on new instance: `cuInit: 0` ✓ before doing any setup

**vLLM version issues encountered:**
1. `vllm==0.15.1` (auto-installed) pulled `torch 2.9.1+cu128` — Marlin AWQ kernel compiled with PTX ISA too new for this driver → `cudaErrorUnsupportedPtxVersion`
2. Fixed by downgrading: `pip install 'vllm==0.7.3'` → `torch 2.5.1+cu124` → CUDA works, AWQ loads cleanly

**vLLM launch quirk:** Multi-line `nohup` command in SSH heredoc caused SSH session to exit 255. Fixed by writing `~/run_vllm.sh` script and running `nohup bash ~/run_vllm.sh > ~/vllm2.log 2>&1 &`.

vLLM 0.7.3 server ready in **80 seconds** (model cached from failed first attempt).

### 14. Full 70B KB Build
`python build_initial_kb.py --skip-ingestion` with `LLM_MODE=local`:

| Phase | Result | Time |
|---|---|---|
| Phase 2: LLM stat inference | 3,570 inferred rows (17 stats × 20 countries) | ~12 min |
| Phase 3: Edge weight inference | 186 cross-border weights | ~11 min |
| Phase 4: Graph construction | 2,220 nodes / 2,545 edges | <1 min |
| Phase 5: HMM Baum-Welch | All 20 countries trained | ~40 sec |
| Phase 6: Snapshot export | `kb_snapshot_20260217.tar.gz` (0.2 MB) | <1 sec |
| **Total** | | **24.0 minutes** |

Snapshot downloaded to `data/build/snapshots/kb_20260217/`. Lambda instance can now be terminated.

### 15. Build Comparison: 8B vs 70B

| Metric | Prior (8B, A100 40GB) | Today (70B, H100 80GB) | Change |
|---|---|---|---|
| API observations | 18,727 | 36,869 | +97% |
| LLM-inferred stats | 2,958 | 3,570 | +21% |
| Total observations | 21,685 | 40,439 | +87% |
| Policy rate countries | 4 | 17 | +325% |
| Active API sources | WB + FRED | WB + FRED + IMF | +1 live |
| Ingestion time | 44.5 min (Lambda throttled) | 75 sec (home machine) | 35× faster |
| LLM inference quality | 8B — terse outputs | 70B — full mechanisms, calibrated confidence | Major |

## Key Files Created/Modified
| File | Action | Notes |
|---|---|---|
| `global_financial_kb/ingestion/pipeline.py` | Rewritten | Serial → parallel async, 32× faster |
| `global_financial_kb/build_initial_kb.py` | Fixed | `--skip-ingestion` standalone bug |
| `global_financial_kb/setup_lambda.sh` | Rewritten | venv, 48GB threshold, H100 target, correct model names |
| `global_financial_kb/processing/llm_interface.py` | Fixed | Default local model → ungated hugging-quants mirror |
| `global_financial_kb/config/settings.py` | Fixed + Updated | DEV_DATA_ROOT path; new stat fields; corrected IMF codes; expanded policy_rate |
| `global_financial_kb/ingestion/fetchers/imf.py` | Created | IMF DataMapper fetcher (live) |
| `global_financial_kb/ingestion/fetchers/oecd.py` | Created | OECD SDMX fetcher (graceful empty) |
| `global_financial_kb/ingestion/fetchers/bis.py` | Created | BIS fetcher (graceful empty) |
| `global_financial_kb/ingestion/fetchers/eia.py` | Created | EIA fetcher (future-ready) |

## Data Locations (Current)
- **Production snapshot:** `data/build/snapshots/kb_20260217/`
  - `financial_kg.json` — 960 KB, 2,220 nodes, 2,545 edges
  - `observations.parquet` — 36,869 API rows
  - `llm_stats.parquet` — 3,570 LLM-inferred rows (70B quality)
  - `edge_weights.parquet` — 186 cross-border edges with mechanisms
  - `hmm_params.json` — 20 country HMM models
- **Raw ingestion:** `data/raw/observations_20260217_203523.parquet` (36,869 rows, 114 KB)
- **Snapshot tar.gz:** `data/build/snapshots/kb_snapshot_20260217.tar.gz` (0.2 MB)

## Key Learnings for Future Lambda Runs
- **Target `gpu_1x_h100_pcie`** (PCIe variant) — H100 SXM5 instance had broken CUDA passthrough
- **CUDA smoke test first** (`cuInit: 0`) before running setup, to avoid wasting time on broken instances
- **vLLM version pin:** `vllm==0.7.3` is stable on H100 PCIe; `vllm==0.15.1` has PTX incompatibility with torch 2.9.1
- **nohup in SSH:** Write a `~/run_vllm.sh` script, then `nohup bash ~/run_vllm.sh > log 2>&1 &` — inline heredoc nohup causes SSH exit 255
- **Pre-fetch locally:** Run `--skip-llm` on home machine first → 75s vs 44+ min on Lambda's throttled IP
- **setup_lambda.sh now includes:** venv creation, correct VRAM threshold (48GB), H100 detection (78GB), 32768 context length

## Pending / Next Session
1. **Terminate Lambda instance** `209.20.158.148` (build complete, snapshot downloaded)
2. **Phase 3 (daily pipeline):** differential KB update + Output A/B/C generators (scanner_signals.json, daily_risk_report.md, threshold alerts)
3. **Phase 4 (market actionability layer):** sector impact mapping + NA proxy resolver
4. **`sec_intelligence.parquet` refresh** — stale since Feb 3
5. **NASDAQ ETF reference files** — missing at `/fast-storage/reference/` (ETF filtering disabled in v29)
6. **Dynamic scenario management** — scenario lifecycle (emerging → active → fading → archived)
7. **OECD API** — monitor `sdmx.oecd.org` migration; update `oecd.py` once correct endpoint format confirmed
8. **BIS API** — update `bis.py` when `WS_CBPOL` data endpoint is fixed server-side
9. **EIA stats** — register `crude_production` etc. in `config/settings.py` with `api_provider="eia"`
8. **BIS API** — monitor `stats.bis.org/api/v2/data/WS_CBPOL` 500 error; update `bis.py` when fixed
9. **EIA** — register `crude_production` or similar stat in `settings.py` with `api_provider="eia"` to activate

## Notes
- World Bank API intermittently times out for India, Russia, Brazil, South Korea — expect errors on daily runs
- US observation count much higher than others — FRED returns daily series, WB returns annual
- The `trading_core` symlink in `trading_systems/` is still not committed to git (excluded as symlink)
- 8B model inference quality: adequate for structured JSON stat inference; not tested for complex reasoning tasks
- IMF DataMapper indicator codes ≠ IMF WEO codes — always verify via `/api/v1/indicators` endpoint first
- IMF DataMapper uses ISO-3 country codes, NOT the numeric "imf" codes in COUNTRY_CODES dict
