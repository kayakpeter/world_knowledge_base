# Session Log — 2026-02-17

## Context
- President's Day (Feb 16) — market closed, no new data for that day
- Trader v29 running with Feb 16 watchlist (7 symbols: FKU, FLQS, GLTO, CNVS, VTAK, GP, FCA)
- Previous session (Feb 16) completed: Neo4j password fix, FastAPI systemd service, PalantirGatekeeper wired into v29, macro context cron job

## Tasks Completed

### 1. World Knowledge Base — Project Exploration
- Extracted `global_financial_kb (1).tar.gz` from `/media/peter/fast-storage/projects/world_knowledge_base/`
- Confirmed codebase: 7,324 lines, 16 files — fully built in a prior session with Gemini/Claude
- Fixed `DEV_DATA_ROOT` in `config/settings.py` (was `/home/claude/...`, now uses project-relative `data/`)
- Installed missing dependency: `hmmlearn`
- Confirmed all modes work: `--mode hmm-test`, `--mode graph-only`, `--mode full`
- Discovered existing `financial_kg.json` (436KB) with 2,220 nodes and 2,398 edges

### 2. Phase 1 — Parallel Async Ingestion (32× speedup)
**Problem:** Original ingestion was serial — `for stat: for country: await fetch()` — estimated 8+ hours for 86 stats × 20 countries. Confirmed by running old version (55 min, only finished 2/86 stats before kill).

**Fix:** Rewrote `ingestion/pipeline.py`:
- `asyncio.gather(*all_tasks)` — all fetches run concurrently
- Per-host semaphores: World Bank=3, FRED=8 concurrent connections
- Timeout: 30s → 15s (fail fast)
- `asyncio.Lock()` for thread-safe accumulation
- Staleness cache: skips re-fetching data < N days old (WB=30 days, FRED=1 day)
- Progress logging every 25 tasks

**Result:** 15,105 observations collected in **14 minutes 57 seconds** (was 8+ hours projected)

### 3. Phase 2 — Lambda Build Script
Created `build_initial_kb.py` — self-contained 6-phase build script for Lambda Labs A6000/A100:
- Phase 1: Full parallel ingestion (`skip_cache=True`)
- Phase 2: LLM stat inference — 17 LLM-required stats × 20 countries, batched (5 stats/call → ~80 API calls vs 340)
- Phase 3: Edge weight inference — 31 trade corridors × 6 stat pairs (~31 calls vs 7,000)
- Phase 4: Full graph construction with LLM-blended contagion weights
- Phase 5: Baum-Welch HMM training per country (all 20 converged)
- Phase 6: Pack `kb_snapshot_YYYYMMDD.tar.gz` for rsync download

Features:
- `--resume` flag: checkpoint saves after each country, survives Lambda preemption
- `--skip-llm`: API-data-only build for testing
- `--skip-ingestion --resume`: use existing parquet, jump to graph phases
- `LLM_MODE=local|claude`: switch between vLLM (Lambda) and Claude API (dev)

Created `setup_lambda.sh` — one-shot Lambda environment setup:
- Installs Python deps + vLLM
- Auto-detects GPU VRAM: A6000/A100 → Llama-3.1-70B-AWQ-INT4, smaller → Mistral-7B
- Starts vLLM server in background, waits for ready (up to 15 min for model download)
- Prints rsync upload/download commands

**Tested:** Build runs end-to-end in 6 seconds locally (--skip-llm, cached data). All 20 countries HMM-trained, snapshot exported.

### 4. Git & GitHub
- Initialized new git repo in `global_financial_kb/`
- Created GitHub repo: **https://github.com/kayakpeter/world_knowledge_base**
- Two commits pushed:
  - `75b1255` — initial commit (Phase 1)
  - `49999dc` — Phase 2 Lambda build

### 5. Architecture Discussion
Had extended design session on the World KB roadmap. Key decisions:
- **Option D** — all three output types:
  - A: `scanner_signals.json` → Greenshoe scanner reads at market open
  - B: `daily_risk_report.md` → human review (email/WhatsApp)
  - C: threshold alerts → push when scenario Δ > 5pp or HMM state changes
- **Two-machine architecture:**
  - Lambda (one-time/weekly): full ingestion + LLM edge weights + Baum-Welch
  - Home system (daily): incremental update, Claude API for small LLM calls, signal generation
- **rsync** for data transfer, Lambda persistent storage for parquet cache
- **Revenue Canada case**: Lambda struggles = documented proof for hardware upgrade (A6000 × 3-4 or A100)
- Future phases discussed: sub-national breakdown, dynamic scenario management, stock exchange integration (North American actionability filter), data source intelligence agent

## Key Files Created/Modified
| File | Action | Notes |
|---|---|---|
| `global_financial_kb/ingestion/pipeline.py` | Rewritten | Serial → parallel async, 32× faster |
| `global_financial_kb/build_initial_kb.py` | Created | Lambda 6-phase build script |
| `global_financial_kb/setup_lambda.sh` | Created | One-shot Lambda environment setup |
| `global_financial_kb/config/settings.py` | Fixed | DEV_DATA_ROOT path |
| `global_financial_kb/.gitignore` | Created | Excludes parquet data, pyc, tar.gz |

## Data Locations
- **Raw observations:** `global_financial_kb/data/raw/observations_20260217_155028.parquet` (76KB, 15,105 rows)
- **Staleness cache:** `global_financial_kb/data/raw/staleness_cache.parquet`
- **Test snapshot:** `global_financial_kb/data/build/snapshots/kb_snapshot_20260217.tar.gz`
- **Episode 001 script:** `global_financial_kb/data/briefings/episode_001/episode_001_script.md`

## HMM State Estimates (Feb 17, sparse data)
Notable even with limited observations:
- Japan → **Crisis** (BoJ policy rate trajectory)
- Canada → **Turbulent** (USMCA/tariff exposure)
- Russia → **Turbulent**
- India → **Turbulent**
- US → Tranquil (but US has 2,245 obs — most reliable estimate)

## Session 2 — Lambda Build Run (same day, continuation)

### 6. Lambda Instance Setup
- Spun up `gpu_1x_a100_sxm4` in us-west-2 (us-east-1 had insufficient capacity)
- Instance ID: `0843f1fa0e3549ba9c4f43c1f875b767` | IP: `129.146.71.49` | Cost: $1.29/hr
- Added `129.146.71.49` to `~/.ssh/known_hosts`
- Uploaded project code via rsync (228 files, excludes parquet/tar.gz/pyc)

### 7. vLLM Environment Fix
**Problem:** `setup_lambda.sh` installed vLLM via `pip install --user`, but system TensorFlow (compiled for numpy 1.x) conflicts with vLLM's numpy 2.x. `transformers/image_transforms.py` unconditionally imports TF → crash on startup.

**Fix:** Created isolated virtualenv (`~/venv`), copied user site-packages into it, ran `pip install vllm ...` inside venv to resolve all deps cleanly. Isolated from system TF.

**70B model OOM:** `hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4` weights consume ~37-38 GB on A100 40GB, leaving no room for vLLM KV cache blocks. Tried both `--gpu-memory-utilization 0.90` (OOM during torch.inductor autotuning compilation) and `0.85 + --enforce-eager` (KV cache check fails — 0 blocks available). **A100 40GB cannot run 70B AWQ + KV cache.**

**Workaround:** Switched to `hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4`:
- ~5 GB weights, 34 GB free for KV cache
- Fits easily, CUDA graphs compiled in 2.5s
- Server healthy at `http://localhost:8000/health`

**Also fixed:** Updated `processing/llm_interface.py` default model from `meta-llama/Llama-3.3-70B-Instruct` → `hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4` on Lambda.

### 8. Full KB Build — Lambda Run
Launched `build_initial_kb.py` with `LLM_MODE=local`, `FRED_API_KEY` set.

| Phase | Result | Time |
|---|---|---|
| Phase 1: Parallel ingestion | 18,727 observations (1,720 tasks) | 44.5 min (WB throttling slowed tail) |
| Phase 2: LLM stat inference | 2,958 LLM-inferred data points | ~2.5 min |
| Phase 3: Edge weight inference | 186 cross-border edges | ~2.5 min |
| Phase 4: Graph construction | 2,220 nodes / 2,552 edges | <1 min |
| Phase 5: HMM Baum-Welch | All 20 countries trained | ~14 sec |
| Phase 6: Snapshot export | `kb_snapshot_20260217.tar.gz` | <1 sec |
| **Total** | | **49.4 minutes / ~$1.06** |

Snapshot downloaded to local machine. Lambda instance terminated.

## Data Locations (Updated)
- **Lambda snapshot (production build):** `/media/peter/fast-storage/projects/world_knowledge_base/snapshots/kb_20260217/`
  - `financial_kg.json` — 980 KB, 2,220 nodes, 2,552 edges
  - `hmm_params.json` — 20 countries trained
  - `observations.parquet` — 18,727 rows
  - `llm_stats.parquet` — 2,958 rows
  - `edge_weights.parquet` — 186 cross-border edges
  - `manifest.json` — build metadata
- **Tar.gz:** `/media/peter/fast-storage/projects/world_knowledge_base/snapshots/kb_snapshot_20260217.tar.gz`
- **Local dev snapshot:** `global_financial_kb/data/build/snapshots/kb_snapshot_20260217.tar.gz`
- **Episode 001 script:** `global_financial_kb/data/briefings/episode_001/episode_001_script.md`

## Key Learnings for Future Lambda Runs
- **70B model requires A6000 48GB or A100 80GB** — A100 40GB cannot hold model + KV cache
- **vLLM must run in a venv** — system TF (numpy 1.x compiled) conflicts with vLLM's numpy 2.x
- **`setup_lambda.sh` needs updates** before next run:
  - Create venv instead of `pip install --user`
  - Use 8B model by default for 40GB instances; 70B for ≥48GB
  - `meta-llama/` models are gated — use `hugging-quants/` mirrors
- **World Bank rate-limits Lambda IPs more aggressively** than home IP — last ~14% of ingestion takes ~30 min instead of ~5 min
- **Lambda API (Cloudflare-protected) blocks programmatic termination from home IP** — use web dashboard

## Pending / Next Session
1. **Phase 3 (daily pipeline):** differential KB update + Output A/B/C generators (scanner_signals.json, daily_risk_report.md, threshold alerts)
2. **Phase 4 (market actionability layer):** sector impact mapping + NA proxy resolver
3. **`sec_intelligence.parquet` refresh** — still stale (Feb 3), many 1970-01-01 dates, risk_score=0
4. **NASDAQ ETF reference files** — still missing at `/fast-storage/reference/` (ETF filtering disabled in v29)
5. **Dynamic scenario management** — scenario lifecycle (emerging → active → fading → archived)
6. **update `setup_lambda.sh`** — venv creation, 8B default model, gated model workaround

## Notes
- World Bank API intermittently times out for India, Russia, Brazil, South Korea — expect errors on daily runs
- US observation count much higher than others — FRED returns daily series, WB returns annual
- The `trading_core` symlink in `trading_systems/` is still not committed to git (excluded as symlink)
- 8B model inference quality: adequate for structured JSON stat inference; not tested for complex reasoning tasks
